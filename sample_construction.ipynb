{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pprint as pprint\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from operator import itemgetter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(\"processed_data/train_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_tokenization = []\n",
    "mismatch_subtokenization = []\n",
    "passage_id_to_id_dict = {}\n",
    "id_to_passageid_dict = {}\n",
    "valid_id_to_original_id_dict = {}\n",
    "train_sent_len_list = []\n",
    "train_passage_list = [] \n",
    "train_tokens_list = []\n",
    "train_embeddings_list = []\n",
    "train_sentence_cum_lengths = []\n",
    "valid_train_docs = 0\n",
    "train_exception_passages = [886, 1990, 2709, 2710, 1687,2200, 2724]\n",
    "negative_samples_factor = 5\n",
    "train_positive_pairs = []\n",
    "train_negative_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed docs =  0\n",
      "processed docs =  100\n",
      "processed docs =  200\n",
      "processed docs =  300\n",
      "processed docs =  400\n",
      "processed docs =  500\n",
      "processed docs =  600\n",
      "processed docs =  700\n",
      "processed docs =  800\n",
      "processed docs =  900\n",
      "processed docs =  1000\n",
      "processed docs =  1100\n",
      "processed docs =  1200\n",
      "processed docs =  1300\n",
      "processed docs =  1400\n",
      "processed docs =  1500\n",
      "processed docs =  1600\n",
      "processed docs =  1700\n",
      "processed docs =  1800\n",
      "processed docs =  1900\n",
      "processed docs =  2000\n",
      "processed docs =  2100\n",
      "processed docs =  2300\n",
      "processed docs =  2400\n",
      "processed docs =  2500\n",
      "processed docs =  2600\n",
      "processed docs =  2700\n",
      "processed docs =  2800\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(train_data):\n",
    "    \n",
    "    if(i in train_exception_passages):\n",
    "        continue\n",
    "    if(i%100 == 0):\n",
    "        print(\"processed docs = \", i)\n",
    "    sentences = doc[0]\n",
    "    tokens = doc[1]\n",
    "    speakers = doc[3]\n",
    "    doc_id = doc[4]\n",
    "    part_id = doc[5]\n",
    "    total_tokens = []\n",
    "    passage_text = \"\"\n",
    "    included_setences = 0\n",
    "    sent_lens = []\n",
    "    previous_length = 0\n",
    "    #get passage text and tokens of 512 length\n",
    "    for sent in sentences:\n",
    "        if(len((passage_text + sent).split()) >= 400):\n",
    "            break\n",
    "        included_setences += 1\n",
    "        passage_text += sent\n",
    "    for token_list in tokens[0:included_setences]:\n",
    "        sent_lens.append(previous_length)\n",
    "        previous_length += len(token_list)\n",
    "        total_tokens += token_list\n",
    "    \n",
    "    #remove mismatch tokens\n",
    "    valid = True\n",
    "    passage_text_processed = nlp(passage_text)\n",
    "    spacy_tokens = []\n",
    "    for w_i, word in enumerate(passage_text_processed):\n",
    "        spacy_tokens += [word]\n",
    "    if(len(spacy_tokens) != len(total_tokens)):\n",
    "        mismatch_tokenization += [i]\n",
    "        valid = False\n",
    "        continue\n",
    "    else:\n",
    "        for (a,b) in zip(total_tokens, spacy_tokens):\n",
    "            if(a.lower() != b.text.lower()): #check at individual tokens level\n",
    "                print(\"mismatch of \", a.lower(), b.text.lower())\n",
    "                mismatch_subtokenization += [i]\n",
    "                valid = False\n",
    "                break\n",
    "    \n",
    "    \n",
    "    #save valid tokensmatc\n",
    "    if(valid):\n",
    "        #bert_embeddings = []\n",
    "        #bert_embeddings = get_bert_embeddings(passage_text)\n",
    "        #assert len(bert_embeddings) == len(total_tokens)\n",
    "        passage_id_to_id_dict.update({doc_id+part_id:valid_train_docs})\n",
    "        id_to_passageid_dict.update({valid_train_docs:doc_id+part_id})\n",
    "        train_sent_len_list.append(included_setences)\n",
    "        train_passage_list.append(passage_text)\n",
    "        train_tokens_list.append(total_tokens)\n",
    "        #train_embeddings_list.append(bert_embeddings)\n",
    "        train_sentence_cum_lengths.append(sent_lens)\n",
    "        valid_id_to_original_id_dict.update({valid_train_docs:i})\n",
    "        valid_train_docs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1189 1613 2802\n"
     ]
    }
   ],
   "source": [
    "skip_passages = len(mismatch_tokenization) + len(train_exception_passages)\n",
    "valid_passages = len(id_to_passageid_dict)\n",
    "print(valid_passages, skip_passages, valid_passages + skip_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_corefs():\n",
    "    train_coref_list = []\n",
    "    skipped = 0\n",
    "    for i in range(valid_passages):\n",
    "        valid_sentences = train_sent_len_list[i]\n",
    "        tokens = train_tokens_list[i]\n",
    "        cumulative_lengths = train_sentence_cum_lengths[i]\n",
    "        original_id = valid_id_to_original_id_dict[i]\n",
    "        corefs_list = train_data[original_id][2][0:valid_sentences]\n",
    "        doc_coref_dict = {}\n",
    "        for (j, sent_corefs_list) in enumerate(corefs_list):\n",
    "            for coref_dicts in sent_corefs_list:\n",
    "                start = int(coref_dicts['start']) + cumulative_lengths[j]\n",
    "                end = int(coref_dicts['end']) + cumulative_lengths[j]\n",
    "                cluster_id = int(coref_dicts['label'])\n",
    "                if start >= len(tokens) or end >= len(tokens):\n",
    "                    skipped += 1\n",
    "                else:\n",
    "                    if cluster_id in doc_coref_dict.keys():\n",
    "                        doc_coref_dict[cluster_id].append((start, end))\n",
    "                    else:\n",
    "                        doc_coref_dict.update({cluster_id:[(start, end)]})\n",
    "        train_coref_list.append(doc_coref_dict)\n",
    "    return train_coref_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_coref_list = parse_corefs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc_coref_list in enumerate(train_coref_list):\n",
    "    for key, value in doc_coref_list.items():\n",
    "        train_coref_list[i][key] = sorted(value, key = lambda x: x[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,doc_coref_list) in enumerate(train_coref_list):\n",
    "    coref_list = []\n",
    "    cluster_ids = list(doc_coref_list.keys())\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    #print(\"number of clusters\", len(cluster_ids))\n",
    "    for key, value in doc_coref_list.items():\n",
    "        for i in range(0, len(doc_coref_list[key])):\n",
    "            for j in range(i+1, len(doc_coref_list[key])):\n",
    "                positive_pairs.append((doc_coref_list[key][i], doc_coref_list[key][j]))\n",
    "    #print(\"number of positive samples\", len(positive_pairs))\n",
    "    negative_samples = len(positive_pairs) * negative_samples_factor\n",
    "    while len(negative_pairs) < negative_samples and len(cluster_ids) > 1:\n",
    "        t_menA_id = cluster_ids[random.randint(0, len(cluster_ids))-1]\n",
    "        t_menB_id = t_menA_id\n",
    "        while t_menB_id == t_menA_id:\n",
    "            t_menB_id = cluster_ids[random.randint(0, len(cluster_ids))-1]\n",
    "        menA_id = min(t_menA_id, t_menB_id)\n",
    "        menB_id = max(t_menA_id, t_menB_id)\n",
    "        a_index = random.randint(0, len(doc_coref_list[menA_id])-1)\n",
    "        b_index = random.randint(0, len(doc_coref_list[menB_id])-1)\n",
    "        menA_id = doc_coref_list[menA_id][a_index]\n",
    "        menB_id = doc_coref_list[menB_id][b_index]\n",
    "        negative_pairs.append((menA_id, menB_id))\n",
    "    train_positive_pairs.append(positive_pairs)\n",
    "    train_negative_pairs.append(negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_positive_pairs.npy\", train_positive_pairs)\n",
    "np.save(\"train_negative_pairs.npy\", train_negative_pairs)\n",
    "np.save(\"train_passage_list.npy\", train_passage_list)\n",
    "np.save(\"valid_id_to_original_id_dict.npy\", valid_id_to_original_id_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
