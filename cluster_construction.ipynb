{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_lg\n",
    "import numpy as np\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVED_CHAR = [\"/\", \"%\", \"*\"]\n",
    "NORMALIZE_DICT = {\"/.\": \".\",\n",
    "                  \"/?\": \"?\",\n",
    "                  \"-LRB-\": \"(\",\n",
    "                  \"-RRB-\": \")\",\n",
    "                  \"-LCB-\": \"{\",\n",
    "                  \"-RCB-\": \"}\",\n",
    "                  \"-LSB-\": \"[\",\n",
    "                    \"-RSB-\": \"]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(token):\n",
    "    cleaned_token = token\n",
    "    if cleaned_token in NORMALIZE_DICT:\n",
    "        cleaned_token = NORMALIZE_DICT[cleaned_token]\n",
    "    if cleaned_token not in REMOVED_CHAR:\n",
    "        for char in REMOVED_CHAR:\n",
    "            cleaned_token = cleaned_token.replace(char, u'')\n",
    "    if len(cleaned_token) == 0:\n",
    "        cleaned_token = \",\"\n",
    "    return cleaned_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_COREF_LIST = [\"i\", \"me\", \"my\", \"you\", \"your\"]\n",
    "\n",
    "MENTION_TYPE = {\"PRONOMINAL\": 0, \"NOMINAL\": 1, \"PROPER\": 2, \"LIST\": 3}\n",
    "MENTION_LABEL = {0: \"PRONOMINAL\", 1: \"NOMINAL\", 2: \"PROPER\", 3: \"LIST\"}\n",
    "\n",
    "PROPERS_TAGS = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "ACCEPTED_ENTS = [\"PERSON\", \"NORP\", \"FACILITY\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LANGUAGE\"]\n",
    "WHITESPACE_PATTERN = r\"\\s+|_+\"\n",
    "UNKNOWN_WORD = \"*UNK*\"\n",
    "MISSING_WORD = \"<missing>\"\n",
    "MAX_ITER = 100\n",
    "debug = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mentions_spans(doc):\n",
    "    '''\n",
    "    Extract potential mentions from a spacy parsed Doc\n",
    "    '''\n",
    "    if debug: print('===== doc ====:', doc)\n",
    "    for c in doc:\n",
    "        if debug: print(\"ðŸš§ span search:\", c, \"head:\", c.head, \"tag:\", c.tag_, \"pos:\", c.pos_, \"dep:\", c.dep_)\n",
    "    # Named entities\n",
    "    mentions_spans = list(ent for ent in doc.ents if ent.label_ in ACCEPTED_ENTS)\n",
    "\n",
    "    if debug: print(\"==-- ents:\", list(((ent, ent.label_) for ent in mentions_spans)))\n",
    "    for sent in doc.sents:\n",
    "        spans, spans_loc = _extract_from_sent(doc, sent, True)\n",
    "        mentions_spans = mentions_spans + spans\n",
    "    spans_set = {}\n",
    "    cleaned_mentions_spans = []\n",
    "    for spans in mentions_spans:\n",
    "        if spans.end > spans.start and (spans.start, spans.end) not in spans_set.values():\n",
    "            cleaned_mentions_spans.append(spans)\n",
    "            spans_set.update({spans:(spans.start, spans.end)})\n",
    "\n",
    "    return cleaned_mentions_spans, spans_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_from_sent(doc, span, blacklist=True, debug=False):\n",
    "    '''\n",
    "    Extract Pronouns and Noun phrases mentions from a spacy Span\n",
    "    '''\n",
    "    keep_tags = re.compile(r\"N.*|PRP.*|DT|IN\")\n",
    "    leave_dep = [\"det\", \"compound\", \"appos\"]\n",
    "    keep_dep = [\"nsubj\", \"dobj\", \"iobj\", \"pobj\"]\n",
    "    nsubj_or_dep = [\"nsubj\", \"dep\"]\n",
    "    conj_or_prep = [\"conj\", \"prep\"]\n",
    "    remove_pos = [\"CCONJ\", \"INTJ\", \"ADP\"]\n",
    "    lower_not_end = [\"'s\", ',', '.', '!', '?', ':', ';']\n",
    "\n",
    "    # Utility to remove bad endings\n",
    "    def cleanup_endings(left, right, token):\n",
    "        minchild_idx = min(left + [token.i]) if left else token.i\n",
    "        maxchild_idx = max(right + [token.i]) if right else token.i\n",
    "        # Clean up endings and begginging\n",
    "        while maxchild_idx >= minchild_idx and (doc[maxchild_idx].pos_ in remove_pos\n",
    "                                           or doc[maxchild_idx].lower_ in lower_not_end):\n",
    "            if debug: print(\"Removing last token\", doc[maxchild_idx].lower_, doc[maxchild_idx].tag_)\n",
    "            maxchild_idx -= 1 # We don't want mentions finishing with 's or conjunctions/punctuation\n",
    "        while minchild_idx <= maxchild_idx and (doc[minchild_idx].pos_ in remove_pos \n",
    "                                           or doc[minchild_idx].lower_ in lower_not_end):\n",
    "            if debug: print(\"Removing first token\", doc[minchild_idx].lower_, doc[minchild_idx].tag_)\n",
    "            minchild_idx += 1 # We don't want mentions starting with 's or conjunctions/punctuation\n",
    "        return minchild_idx, maxchild_idx+1\n",
    "\n",
    "    mentions_spans = []\n",
    "    mention_spans_loc = []\n",
    "    for token in span:\n",
    "        if debug: print(\"ðŸš€ tok:\", token, \"tok.tag_:\", token.tag_, \"tok.pos_:\", token.pos_, \"tok.dep_:\", token.dep_)\n",
    "\n",
    "        if blacklist and token.lower_ in NO_COREF_LIST:\n",
    "            if debug: print(\"token in no_coref_list\")\n",
    "            continue\n",
    "        if (not keep_tags.match(token.tag_) or token.dep_ in leave_dep) and not token.dep_ in keep_dep:\n",
    "            if debug: print(\"not pronoun or no right dependency\")\n",
    "            continue\n",
    "\n",
    "        # pronoun\n",
    "        if re.match(r\"PRP.*\", token.tag_):\n",
    "            if debug: print(\"PRP\")\n",
    "            endIdx = token.i + 1\n",
    "\n",
    "            span = doc[token.i: endIdx]\n",
    "            if debug: print(\"==-- PRP store:\", span)\n",
    "            mentions_spans.append(span)\n",
    "            mention_spans_loc.append((token.i, endIdx))\n",
    "\n",
    "            # when pronoun is a part of conjunction (e.g., you and I)\n",
    "            if token.n_rights > 0 or token.n_lefts > 0:\n",
    "                span = doc[token.left_edge.i : token.right_edge.i+1]\n",
    "                if debug: print(\"==-- in conj store:\", span)\n",
    "                mentions_spans.append(span)\n",
    "                mention_spans_loc.append((token.left_edge.i, token.right_edge.i+1))\n",
    "            continue\n",
    "\n",
    "        # Add NP mention\n",
    "        if debug:\n",
    "            print(\"NP or IN:\", token.lower_)\n",
    "            if token.tag_ == 'IN':\n",
    "                print(\"IN tag\")\n",
    "        # Take care of 's\n",
    "        if token.lower_ == \"'s\":\n",
    "            if debug: print(\"'s detected\")\n",
    "            h = token.head\n",
    "            j = 0\n",
    "            while h.head.i != h.i and j < MAX_ITER:\n",
    "                if debug:\n",
    "                    print(\"token head:\", h, h.dep_, \"head:\", h.head)\n",
    "                    print(id(h.head), id(h))\n",
    "                if h.dep_ == \"nsubj\":\n",
    "                    minchild_idx = min((c.left_edge.i for c in doc if c.head.i == h.head.i and c.dep_ in nsubj_or_dep),\n",
    "                                       default=token.i)\n",
    "                    maxchild_idx = max((c.right_edge.i for c in doc if c.head.i == h.head.i and c.dep_ in nsubj_or_dep),\n",
    "                                       default=token.i)\n",
    "                    if debug: print(\"'s', i1:\", doc[minchild_idx], \" i2:\", doc[maxchild_idx])\n",
    "                    span = doc[minchild_idx : maxchild_idx+1]\n",
    "                    if debug: print(\"==-- 's' store:\", span)\n",
    "                    mentions_spans.append(span)\n",
    "                    mention_spans_loc.append((minchild_idx, maxchild_idx+1))\n",
    "                    break\n",
    "                h = h.head\n",
    "                j += 1\n",
    "            assert j != MAX_ITER\n",
    "            continue\n",
    "\n",
    "        # clean up\n",
    "        for c in doc:\n",
    "            if debug and c.head.i == token.i: print(\"ðŸš§ token in span:\", c, \"- head & dep:\", c.head, c.dep_)\n",
    "        left = list(c.left_edge.i for c in doc if c.head.i == token.i)\n",
    "        right = list(c.right_edge.i for c in doc if c.head.i == token.i)\n",
    "        if token.tag_ == 'IN' and token.dep_ == \"mark\" and len(left) == 0 and len(right) == 0:\n",
    "            left = list(c.left_edge.i for c in doc if c.head.i == token.head.i)\n",
    "            right = list(c.right_edge.i for c in doc if c.head.i == token.head.i)\n",
    "        if debug:\n",
    "            print(\"left side:\", left)\n",
    "            print(\"right side:\", right)\n",
    "            minchild_idx = min(left) if left else token.i\n",
    "            maxchild_idx = max(right) if right else token.i\n",
    "            print(\"full span:\", doc[minchild_idx:maxchild_idx+1])\n",
    "        start, end = cleanup_endings(left, right, token)\n",
    "        if start == end:\n",
    "            continue\n",
    "        if doc[start].lower_ == \"'s\":\n",
    "            continue # we probably already have stored this mention\n",
    "        span = doc[start:end]\n",
    "        if debug:\n",
    "            print(\"cleaned endings span:\", doc[start:end])\n",
    "            print(\"==-- full span store:\", span)\n",
    "        mentions_spans.append(span)\n",
    "        mention_spans_loc.append((start, end))\n",
    "        if debug and token.tag_ == 'IN':\n",
    "            print(\"IN tag\")\n",
    "        if any(tok.dep_ in conj_or_prep for tok in span):\n",
    "            if debug: print(\"Conjunction found, storing first element separately\")\n",
    "            for c in doc:\n",
    "                if c.head.i == token.i and c.dep_ not in conj_or_prep:\n",
    "                    if debug: print(\"left no conj:\", c, 'dep & edge:', c.dep_, c.left_edge)\n",
    "                    if debug: print(\"right no conj:\", c, 'dep & edge:', c.dep_, c.right_edge)\n",
    "            left_no_conj = list(c.left_edge.i for c in doc if c.head.i == token.i and c.dep_ not in conj_or_prep)\n",
    "            right_no_conj = list(c.right_edge.i for c in doc if c.head.i == token.i and c.dep_ not in conj_or_prep)\n",
    "            if debug: print(\"left side no conj:\", [doc[i] for i in left_no_conj])\n",
    "            if debug: print(\"right side no conj:\", [doc[i] for i in right_no_conj])\n",
    "            start, end = cleanup_endings(left_no_conj, right_no_conj, token)\n",
    "            if start == end:\n",
    "                continue\n",
    "            span = doc[start:end]\n",
    "            if debug: print(\"==-- full span store:\", span)\n",
    "            mentions_spans.append(span)\n",
    "            mention_spans_loc.append((start, end))\n",
    "    if debug: print(\"mentions_spans inside\", mentions_spans)\n",
    "    return mentions_spans, mention_spans_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Manoj, He, he, college],\n",
       " {Manoj: (2, 3), He: (0, 1), he: (4, 5), college: (7, 8)})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('He is Manoj, he went to college')\n",
    "extract_mentions_spans(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_passages = np.load(\"processed_data/test_passage_list.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mentions_list = []\n",
    "for passage in test_passages:\n",
    "    doc = nlp(str(passage))\n",
    "    spans, loc_dict = extract_mentions_spans(doc)\n",
    "    test_mentions_list.append(loc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_passages[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Yang Yang: (59, 61),\n",
       " the Beijing Municipal Construction Commission: (189, 194),\n",
       " the Communication Commission: (197, 200),\n",
       " Traffic: (0, 1),\n",
       " the first thing to be impacted when this , cave - in accident occurred: (2,\n",
       "  16),\n",
       " this: (9, 10),\n",
       " cave: (11, 12),\n",
       " cave - in accident: (11, 15),\n",
       " this accident: (19, 21),\n",
       " many underground pipes , such as sewage pipes , ah , or water , ah , and gas , ah , pipes: (23,\n",
       "  45),\n",
       " many underground pipes: (23, 26),\n",
       " such as sewage pipes , ah , or water , ah , and gas , ah , pipes: (27, 45),\n",
       " sewage pipes , ah , or water , ah , and gas , ah , pipes: (29, 45),\n",
       " sewage pipes: (29, 31),\n",
       " water , ah , and gas: (35, 41),\n",
       " water: (35, 36),\n",
       " gas: (40, 41),\n",
       " water , ah , and gas , ah , pipes: (35, 45),\n",
       " that: (46, 47),\n",
       " the lives of citizens: (48, 52),\n",
       " the lives: (48, 50),\n",
       " citizens: (51, 52),\n",
       " many other aspects: (53, 56),\n",
       " how many pipes: (65, 68),\n",
       " seven main types of pipes buried underground: (78, 85),\n",
       " seven main types: (78, 81),\n",
       " pipes buried underground: (82, 85),\n",
       " They: (86, 87),\n",
       " those like you just mentioned , as well as pipes like those for heating and communication: (88,\n",
       "  104),\n",
       " those like you just mentioned , as well: (88, 96),\n",
       " as well: (94, 96),\n",
       " pipes like those for heating and communication: (97, 104),\n",
       " those for heating and communication: (99, 104),\n",
       " those: (99, 100),\n",
       " heating and communication: (101, 104),\n",
       " heating: (101, 102),\n",
       " communication: (103, 104),\n",
       " others: (106, 107),\n",
       " these pipes: (110, 112),\n",
       " the lives of citizens: (116, 120),\n",
       " the lives: (116, 118),\n",
       " citizens: (119, 120),\n",
       " our attention to the traffic issue: (128, 134),\n",
       " our: (128, 129),\n",
       " our attention: (128, 130),\n",
       " the traffic issue: (131, 134),\n",
       " people: (137, 138),\n",
       " the situation after the accident: (146, 151),\n",
       " the situation: (146, 148),\n",
       " the accident: (149, 151),\n",
       " other aspects affecting citizens ' lives: (153, 159),\n",
       " citizens ': (156, 158),\n",
       " citizens ' lives: (156, 159),\n",
       " 's: (161, 162),\n",
       " a look: (163, 165),\n",
       " this footage of the situation of dealing with the current scene: (167, 178),\n",
       " this footage: (167, 169),\n",
       " the situation of dealing with the current scene: (170, 178),\n",
       " the situation: (170, 172),\n",
       " dealing with the current scene: (173, 178),\n",
       " the current scene: (175, 178),\n",
       " our understanding of the situation: (183, 188),\n",
       " our: (183, 184),\n",
       " our understanding: (183, 185),\n",
       " the situation: (186, 188),\n",
       " the Communication Commission , ah , and telecommunication , traffic , and other departments: (197,\n",
       "  211),\n",
       " telecommunication , traffic , and other departments: (204, 211),\n",
       " telecommunication: (204, 205),\n",
       " traffic , and other departments: (206, 211),\n",
       " traffic: (206, 207),\n",
       " other departments: (209, 211),\n",
       " personnel: (213, 214),\n",
       " emergency repair work: (217, 220),\n",
       " Relevant departments: (221, 223),\n",
       " they will adopt a full range of measures to minimize the level of impact on people 's daily lives and work , in order to ensure stable order in society during the recovery period: (225,\n",
       "  259),\n",
       " they: (225, 226),\n",
       " a full range of measures to minimize the level of impact on people 's daily lives and work: (228,\n",
       "  246),\n",
       " a full range: (228, 231),\n",
       " measures to minimize the level of impact on people 's daily lives and work: (232,\n",
       "  246),\n",
       " the level of impact on people 's daily lives and work: (235, 246),\n",
       " the level: (235, 237),\n",
       " impact: (238, 239),\n",
       " people 's daily lives and work: (240, 246),\n",
       " people: (240, 241),\n",
       " people 's daily lives: (240, 244),\n",
       " work: (245, 246),\n",
       " order to ensure stable order in society during the recovery period: (248,\n",
       "  259),\n",
       " stable order in society: (251, 255),\n",
       " stable order: (251, 253),\n",
       " society: (254, 255),\n",
       " the recovery period: (256, 259),\n",
       " the government 's response to this cave - in accident: (264, 274),\n",
       " the government: (264, 266),\n",
       " the government 's response: (264, 268),\n",
       " this cave - in accident: (269, 274),\n",
       " cave - in: (270, 273),\n",
       " we: (277, 278),\n",
       " it: (280, 281),\n",
       " traffic management and emergency repair: (285, 290),\n",
       " traffic management: (285, 287),\n",
       " emergency repair: (288, 290),\n",
       " it involved various aspects of life: (294, 300),\n",
       " it: (294, 295),\n",
       " various aspects of life: (296, 300),\n",
       " various aspects: (296, 298),\n",
       " life: (299, 300)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_mentions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([word for word in nlp(str(test_passages[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7f8adb347f48>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
