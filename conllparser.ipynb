{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVED_CHAR = [\"/\", \"%\", \"*\"]\n",
    "NORMALIZE_DICT = {\"/.\": \".\",\n",
    "                  \"/?\": \"?\",\n",
    "                  \"-LRB-\": \"(\",\n",
    "                  \"-RRB-\": \")\",\n",
    "                  \"-LCB-\": \"{\",\n",
    "                  \"-RCB-\": \"}\",\n",
    "                  \"-LSB-\": \"[\",\n",
    "                    \"-RSB-\": \"]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(token):\n",
    "    cleaned_token = token\n",
    "    if cleaned_token in NORMALIZE_DICT:\n",
    "        cleaned_token = NORMALIZE_DICT[cleaned_token]\n",
    "    if cleaned_token not in REMOVED_CHAR:\n",
    "        for char in REMOVED_CHAR:\n",
    "            cleaned_token = cleaned_token.replace(char, u'')\n",
    "    if len(cleaned_token) == 0:\n",
    "        cleaned_token = \",\"\n",
    "    return cleaned_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(full_name, debug=False):\n",
    "    '''\n",
    "    load a *._conll file\n",
    "    Input: full_name: path to the file\n",
    "    Output: list of tuples for each conll doc in the file, where the tuple contains:\n",
    "        (utts_text ([str]): list of the utterances in the document \n",
    "         utts_tokens ([[str]]): list of the tokens (conll words) in the document \n",
    "         utts_corefs: list of coref objects (dicts) with the following properties:\n",
    "            coref['label']: id of the coreference cluster,\n",
    "            coref['start']: start index (index of first token in the utterance),\n",
    "            coref['end': end index (index of last token in the utterance).\n",
    "         utts_speakers ([str]): list of the speaker associated to each utterances in the document \n",
    "         name (str): name of the document\n",
    "         part (str): part of the document\n",
    "        )\n",
    "    '''\n",
    "    docs = []\n",
    "    with io.open(full_name, 'rt', encoding='utf-8', errors='strict') as f:\n",
    "        lines = list(f)#.readlines()\n",
    "        utts_text = []\n",
    "        utts_tokens = []\n",
    "        utts_corefs = []\n",
    "        utts_speakers = []\n",
    "        tokens = []\n",
    "        corefs = []\n",
    "        index = 0\n",
    "        speaker = \"\"\n",
    "        name = \"\"\n",
    "        part = \"\"\n",
    "        #print(\"********************only once ********************************\")\n",
    "        for li, line in enumerate(lines):\n",
    "            cols = line.split()\n",
    "            if debug: print(\"line\", li, \"cols:\", cols)\n",
    "            # End of utterance\n",
    "            if len(cols) == 0:\n",
    "                if tokens:\n",
    "                    if debug: print(\"End of utterance\")\n",
    "                    utts_text.append(u''.join(t + u' ' for t in tokens))\n",
    "                    utts_tokens.append(tokens)\n",
    "                    utts_speakers.append(speaker)\n",
    "                    utts_corefs.append(corefs)\n",
    "                    tokens = []\n",
    "                    corefs = []\n",
    "                    #print(\"index reset cols = 0 \", index)\n",
    "                    index = 0\n",
    "                    speaker = \"\"\n",
    "                    continue\n",
    "            # End of doc\n",
    "            elif len(cols) == 2:\n",
    "                if debug: print(\"End of doc\")\n",
    "                if cols[0] == \"#end\":\n",
    "                    if debug: print(\"Saving doc\")\n",
    "                    docs.append((utts_text, utts_tokens, utts_corefs, utts_speakers, name, part))\n",
    "                    utts_text = []\n",
    "                    utts_tokens = []\n",
    "                    utts_corefs = []\n",
    "                    utts_speakers = []\n",
    "                else:\n",
    "                    raise ValueError(\"Error on end line \" + line)\n",
    "            # New doc\n",
    "            elif len(cols) == 5:\n",
    "                if debug: print(\"New doc\")\n",
    "                if cols[0] == \"#begin\":\n",
    "                    name = re.match(r\"\\((.*)\\);\", cols[2]).group(1)\n",
    "                    try:\n",
    "                        part = cols[4]\n",
    "                    except ValueError:\n",
    "                        print(\"Error parsing document part \" + line)\n",
    "                    if debug: print(\"New doc\", name, part, name[:2])\n",
    "                    tokens = []\n",
    "                    corefs = []\n",
    "                    #print(\"index reset\")\n",
    "                    index = 0\n",
    "                else:\n",
    "                    raise ValueError(\"Error on begin line \" + line)\n",
    "            # Inside utterance\n",
    "            elif len(cols) > 7:\n",
    "                if debug: print(\"Inside utterance\")\n",
    "                assert (cols[0] == name and int(cols[1]) == int(part)), \"Doc name or part error \" + line\n",
    "                assert (int(cols[2]) == index), \"Index error on \" + line\n",
    "                if speaker:\n",
    "                    assert (cols[9] == speaker), \"Speaker changed in \" + line + speaker\n",
    "                else:\n",
    "                    speaker = cols[9]\n",
    "                    if debug: print(\"speaker\", speaker)\n",
    "                if cols[-1] != u'-':\n",
    "                    coref_expr = cols[-1].split(u'|')\n",
    "                    if debug: print(\"coref_expr\", coref_expr)\n",
    "                    if not coref_expr:\n",
    "                        raise ValueError(\"Coref expression empty \" + line)\n",
    "                    for tok in coref_expr:\n",
    "                        if debug: print(\"coref tok\", tok)\n",
    "                        try:\n",
    "                            match = re.match(r\"^(\\(?)(\\d+)(\\)?)$\", tok)\n",
    "                        except:\n",
    "                            print(\"error getting coreferences for line \" + line)\n",
    "                        assert match is not None, \"Error parsing coref \" + tok + \" in \" + line\n",
    "                        num = match.group(2)\n",
    "                        assert (num is not u''), \"Error parsing coref \" + tok + \" in \" + line\n",
    "                        if match.group(1) == u'(':\n",
    "                            if debug: print(\"New coref\", num)\n",
    "                            corefs.append({'label': num, 'start': index, 'end': None})\n",
    "                        if match.group(3) == u')':\n",
    "                            j = None\n",
    "                            for i in range(len(corefs)-1, -1, -1):\n",
    "                                if debug: print(\"i\", i)\n",
    "                                if corefs[i]['label'] == num and corefs[i]['end'] is None:\n",
    "                                    j = i\n",
    "                                    break\n",
    "                            assert (j is not None), \"coref closing error \" + line\n",
    "                            if debug: print(\"End coref\", num)\n",
    "                            corefs[j]['end'] = index\n",
    "                tokens.append(clean_token(cols[3]))\n",
    "                index += 1\n",
    "                #print(\"index manoj\",index)\n",
    "            else:\n",
    "                raise ValueError(\"Line not standard \" + line)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/forcerequestspring19_gmail_com/neural/neural-coref/conll_data/train.english.v4_gold_conll\"\n",
    "test_path  = \"/home/forcerequestspring19_gmail_com/neural/neural-coref/conll_data/test.english.v4_gold_conll\"\n",
    "dev_path   = \"/home/forcerequestspring19_gmail_com/neural/neural-coref/conll_data/dev.english.v4_gold_conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data =  load_file(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  load_file(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save(\"train_data.npy\", train_data)\n",
    "np.save(\"test_data.npy\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_COREF_LIST = [\"i\", \"me\", \"my\", \"you\", \"your\"]\n",
    "\n",
    "MENTION_TYPE = {\"PRONOMINAL\": 0, \"NOMINAL\": 1, \"PROPER\": 2, \"LIST\": 3}\n",
    "MENTION_LABEL = {0: \"PRONOMINAL\", 1: \"NOMINAL\", 2: \"PROPER\", 3: \"LIST\"}\n",
    "\n",
    "PROPERS_TAGS = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "ACCEPTED_ENTS = [\"PERSON\", \"NORP\", \"FACILITY\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LANGUAGE\"]\n",
    "WHITESPACE_PATTERN = r\"\\s+|_+\"\n",
    "UNKNOWN_WORD = \"*UNK*\"\n",
    "MISSING_WORD = \"<missing>\"\n",
    "MAX_ITER = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mentions_spans(doc):\n",
    "    '''\n",
    "    Extract potential mentions from a spacy parsed Doc\n",
    "    '''\n",
    "    if debug: print('===== doc ====:', doc)\n",
    "    for c in doc:\n",
    "        if debug: print(\"ðŸš§ span search:\", c, \"head:\", c.head, \"tag:\", c.tag_, \"pos:\", c.pos_, \"dep:\", c.dep_)\n",
    "    # Named entities\n",
    "    mentions_spans = list(ent for ent in doc.ents if ent.label_ in ACCEPTED_ENTS)\n",
    "\n",
    "    if debug: print(\"==-- ents:\", list(((ent, ent.label_) for ent in mentions_spans)))\n",
    "    for sent in doc.sents:\n",
    "        spans = _extract_from_sent(doc, sent, True)\n",
    "        mentions_spans = mentions_spans + spans\n",
    "    spans_set = set()\n",
    "    cleaned_mentions_spans = []\n",
    "    for spans in mentions_spans:\n",
    "        if spans.end > spans.start and (spans.start, spans.end) not in spans_set:\n",
    "            cleaned_mentions_spans.append(spans)\n",
    "            spans_set.add((spans.start, spans.end))\n",
    "\n",
    "    return cleaned_mentions_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_from_sent(doc, span, blacklist=True, debug=False):\n",
    "    '''\n",
    "    Extract Pronouns and Noun phrases mentions from a spacy Span\n",
    "    '''\n",
    "    keep_tags = re.compile(r\"N.*|PRP.*|DT|IN\")\n",
    "    leave_dep = [\"det\", \"compound\", \"appos\"]\n",
    "    keep_dep = [\"nsubj\", \"dobj\", \"iobj\", \"pobj\"]\n",
    "    nsubj_or_dep = [\"nsubj\", \"dep\"]\n",
    "    conj_or_prep = [\"conj\", \"prep\"]\n",
    "    remove_pos = [\"CCONJ\", \"INTJ\", \"ADP\"]\n",
    "    lower_not_end = [\"'s\", ',', '.', '!', '?', ':', ';']\n",
    "\n",
    "    # Utility to remove bad endings\n",
    "    def cleanup_endings(left, right, token):\n",
    "        minchild_idx = min(left + [token.i]) if left else token.i\n",
    "        maxchild_idx = max(right + [token.i]) if right else token.i\n",
    "        # Clean up endings and begginging\n",
    "        while maxchild_idx >= minchild_idx and (doc[maxchild_idx].pos_ in remove_pos\n",
    "                                           or doc[maxchild_idx].lower_ in lower_not_end):\n",
    "            if debug: print(\"Removing last token\", doc[maxchild_idx].lower_, doc[maxchild_idx].tag_)\n",
    "            maxchild_idx -= 1 # We don't want mentions finishing with 's or conjunctions/punctuation\n",
    "        while minchild_idx <= maxchild_idx and (doc[minchild_idx].pos_ in remove_pos \n",
    "                                           or doc[minchild_idx].lower_ in lower_not_end):\n",
    "            if debug: print(\"Removing first token\", doc[minchild_idx].lower_, doc[minchild_idx].tag_)\n",
    "            minchild_idx += 1 # We don't want mentions starting with 's or conjunctions/punctuation\n",
    "        return minchild_idx, maxchild_idx+1\n",
    "\n",
    "    mentions_spans = []\n",
    "    for token in span:\n",
    "        if debug: print(\"ðŸš€ tok:\", token, \"tok.tag_:\", token.tag_, \"tok.pos_:\", token.pos_, \"tok.dep_:\", token.dep_)\n",
    "\n",
    "        if blacklist and token.lower_ in NO_COREF_LIST:\n",
    "            if debug: print(\"token in no_coref_list\")\n",
    "            continue\n",
    "        if (not keep_tags.match(token.tag_) or token.dep_ in leave_dep) and not token.dep_ in keep_dep:\n",
    "            if debug: print(\"not pronoun or no right dependency\")\n",
    "            continue\n",
    "\n",
    "        # pronoun\n",
    "        if re.match(r\"PRP.*\", token.tag_):\n",
    "            if debug: print(\"PRP\")\n",
    "            endIdx = token.i + 1\n",
    "\n",
    "            span = doc[token.i: endIdx]\n",
    "            if debug: print(\"==-- PRP store:\", span)\n",
    "            mentions_spans.append(span)\n",
    "\n",
    "            # when pronoun is a part of conjunction (e.g., you and I)\n",
    "            if token.n_rights > 0 or token.n_lefts > 0:\n",
    "                span = doc[token.left_edge.i : token.right_edge.i+1]\n",
    "                if debug: print(\"==-- in conj store:\", span)\n",
    "                mentions_spans.append(span)\n",
    "            continue\n",
    "\n",
    "        # Add NP mention\n",
    "        if debug:\n",
    "            print(\"NP or IN:\", token.lower_)\n",
    "            if token.tag_ == 'IN':\n",
    "                print(\"IN tag\")\n",
    "        # Take care of 's\n",
    "        if token.lower_ == \"'s\":\n",
    "            if debug: print(\"'s detected\")\n",
    "            h = token.head\n",
    "            j = 0\n",
    "            while h.head.i != h.i and j < MAX_ITER:\n",
    "                if debug:\n",
    "                    print(\"token head:\", h, h.dep_, \"head:\", h.head)\n",
    "                    print(id(h.head), id(h))\n",
    "                if h.dep_ == \"nsubj\":\n",
    "                    minchild_idx = min((c.left_edge.i for c in doc if c.head.i == h.head.i and c.dep_ in nsubj_or_dep),\n",
    "                                       default=token.i)\n",
    "                    maxchild_idx = max((c.right_edge.i for c in doc if c.head.i == h.head.i and c.dep_ in nsubj_or_dep),\n",
    "                                       default=token.i)\n",
    "                    if debug: print(\"'s', i1:\", doc[minchild_idx], \" i2:\", doc[maxchild_idx])\n",
    "                    span = doc[minchild_idx : maxchild_idx+1]\n",
    "                    if debug: print(\"==-- 's' store:\", span)\n",
    "                    mentions_spans.append(span)\n",
    "                    break\n",
    "                h = h.head\n",
    "                j += 1\n",
    "            assert j != MAX_ITER\n",
    "            continue\n",
    "\n",
    "        # clean up\n",
    "        for c in doc:\n",
    "            if debug and c.head.i == token.i: print(\"ðŸš§ token in span:\", c, \"- head & dep:\", c.head, c.dep_)\n",
    "        left = list(c.left_edge.i for c in doc if c.head.i == token.i)\n",
    "        right = list(c.right_edge.i for c in doc if c.head.i == token.i)\n",
    "        if token.tag_ == 'IN' and token.dep_ == \"mark\" and len(left) == 0 and len(right) == 0:\n",
    "            left = list(c.left_edge.i for c in doc if c.head.i == token.head.i)\n",
    "            right = list(c.right_edge.i for c in doc if c.head.i == token.head.i)\n",
    "        if debug:\n",
    "            print(\"left side:\", left)\n",
    "            print(\"right side:\", right)\n",
    "            minchild_idx = min(left) if left else token.i\n",
    "            maxchild_idx = max(right) if right else token.i\n",
    "            print(\"full span:\", doc[minchild_idx:maxchild_idx+1])\n",
    "        start, end = cleanup_endings(left, right, token)\n",
    "        if start == end:\n",
    "            continue\n",
    "        if doc[start].lower_ == \"'s\":\n",
    "            continue # we probably already have stored this mention\n",
    "        span = doc[start:end]\n",
    "        if debug:\n",
    "            print(\"cleaned endings span:\", doc[start:end])\n",
    "            print(\"==-- full span store:\", span)\n",
    "        mentions_spans.append(span)\n",
    "        if debug and token.tag_ == 'IN':\n",
    "            print(\"IN tag\")\n",
    "        if any(tok.dep_ in conj_or_prep for tok in span):\n",
    "            if debug: print(\"Conjunction found, storing first element separately\")\n",
    "            for c in doc:\n",
    "                if c.head.i == token.i and c.dep_ not in conj_or_prep:\n",
    "                    if debug: print(\"left no conj:\", c, 'dep & edge:', c.dep_, c.left_edge)\n",
    "                    if debug: print(\"right no conj:\", c, 'dep & edge:', c.dep_, c.right_edge)\n",
    "            left_no_conj = list(c.left_edge.i for c in doc if c.head.i == token.i and c.dep_ not in conj_or_prep)\n",
    "            right_no_conj = list(c.right_edge.i for c in doc if c.head.i == token.i and c.dep_ not in conj_or_prep)\n",
    "            if debug: print(\"left side no conj:\", [doc[i] for i in left_no_conj])\n",
    "            if debug: print(\"right side no conj:\", [doc[i] for i in right_no_conj])\n",
    "            start, end = cleanup_endings(left_no_conj, right_no_conj, token)\n",
    "            if start == end:\n",
    "                continue\n",
    "            span = doc[start:end]\n",
    "            if debug: print(\"==-- full span store:\", span)\n",
    "            mentions_spans.append(span)\n",
    "    if debug: print(\"mentions_spans inside\", mentions_spans)\n",
    "    return mentions_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== doc ====: He is Manoj, he went to college\n",
      "ðŸš§ span search: He head: is tag: PRP pos: PRON dep: nsubj\n",
      "ðŸš§ span search: is head: went tag: VBZ pos: VERB dep: ccomp\n",
      "ðŸš§ span search: Manoj head: is tag: NNP pos: PROPN dep: attr\n",
      "ðŸš§ span search: , head: went tag: , pos: PUNCT dep: punct\n",
      "ðŸš§ span search: he head: went tag: PRP pos: PRON dep: nsubj\n",
      "ðŸš§ span search: went head: went tag: VBD pos: VERB dep: ROOT\n",
      "ðŸš§ span search: to head: went tag: IN pos: ADP dep: prep\n",
      "ðŸš§ span search: college head: to tag: NN pos: NOUN dep: pobj\n",
      "==-- ents: [(Manoj, 'GPE')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Manoj, He, he, college]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('He is Manoj, he went to college')\n",
    "extract_mentions_spans(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
